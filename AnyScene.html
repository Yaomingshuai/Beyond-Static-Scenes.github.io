<!DOCTYPE html>
<!-- saved from url=(0050)file:///Users/yms/Documents/AnyScene-v1/index.html -->
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="description" content="AnyScene">
  <!-- <meta name="keywords" content="Virtual Try-on, Customizable Generation, Diffusion Model"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Beyond Static Scenes</title>


  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style><link href="./AnyScene_files/css" rel="stylesheet">

  <link rel="stylesheet" href="./AnyScene_files/bulma.min.css">
  <link rel="stylesheet" href="./AnyScene_files/bulma-carousel.min.css">
  <link rel="stylesheet" href="./AnyScene_files/bulma-slider.min.css">
  <link rel="stylesheet" href="./AnyScene_files/fontawesome.all.min.css">
  <link rel="stylesheet" href="./AnyScene_files/academicons.min.css">
  <link rel="stylesheet" href="./AnyScene_files/index.css">
  <link rel="icon" href="file:///Users/yms/Documents/AnyScene-v1/static/images/icon.png">

  <script src="./AnyScene_files/jquery.min.js"></script>
  <script defer="" src="./AnyScene_files/fontawesome.all.min.js"></script>
  <script src="./AnyScene_files/bulma-carousel.min.js"></script>
  <script src="./AnyScene_files/bulma-slider.min.js"></script>
  <script src="./AnyScene_files/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
</div></nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Beyond Static Scenes: Camera-controllable Background Generation for Human Motion</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Mingshuai Yao<sup>1 <sup> </a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Mengting Chen<sup>2<sup> </a>&nbsp</a>&nbsp  
            </span>
            
            <span class="author-block">
              Qinye Zhou<sup>2<sup>  </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Yabo Zhang<sup>2</sup>  </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Ming Liu<sup>2</sup>  </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Xiaoming Li<sup>1</sup>  </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Shaohui Liu<sup>2</sup>   </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Chen Ju<sup>2</sup>   </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Shuai Xiao<sup>1</sup>   </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Qingwen Liu<sup>1</sup>   </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Jinsong Lan<sup>1</sup>   </a></a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              Wangmeng Zuo<sup>1</sup>   </a></a>&nbsp</a>&nbsp  
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science and Technology</a>&nbsp</a>&nbsp  </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup>Alibaba Group</a>&nbsp</a>&nbsp  </span>
            
          </div> -->
<!-- 
          <div class="column has-text-centered">
            <div class="publication-links"> -->
              <!-- PDF Link. -->
      <!--         <span class="link-block">
                <a href=   "https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
                </a>
            </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
      <div class="container">
          <div class="item item-steve">
          <img src="./generate_images/images/teaser/fig_3_teaser_v3.png"  width="100%">
        </div>
      </div>
    </div>
  </section> -->


<section class="section">
  <div class="container is-max-desktop">
                <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3" style="text-align: center;">Architecture</h2>
                  <div class="content has-text-justified">
                    <p>
                      In this paper, we investigate the generation of new video backgrounds given a human foreground video, a camera pose, and a reference scene image. This task presents three key challenges. First, the generated background should precisely follow the camera movements corresponding to the human foreground. Second, as the camera shifts in different directions, newly revealed content should appear seamless and natural. Third, objects within the video frame should maintain consistent textures as the camera moves to ensure visual coherence. To address these challenges, we propose DynaScene, a new framework that uses camera poses extracted from the original video as an explicit control to drive background motion. Specifically, we design a multi-task learning paradigm that incorporates auxiliary tasks, namely background outpainting and scene variation, to enhance the realism of the generated backgrounds. Given the scarcity of suitable data, we constructed a large-scale, high-quality dataset tailored for this task, comprising video foregrounds, reference scene images, and corresponding camera poses. This dataset contains 200K video clips, ten times larger than existing real-world human video datasets, providing a significantly richer and more diverse training resource. 
                    </p>
                  </div>
                <div class="columns is-centered has-text-centered">
                  <div class="column">
                    <div class="item item-steve">
                      <img src="./AnyScene_files/method.png" width="80%">
                    </div>   
                      
                  </div>
                </div>

         <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <h2 class="title is-3" style="text-align: center;">Adapting to Varied Camera-Person Relationships</h2> -->
<!--               <div class="content has-text-justified">
                <p>
                  Tunnel Try-on can not only handle complex clothing and backgrounds but also adapt to different types of movements in the video.
                </p>
              </div> -->
            <!-- <h2 class="title is-4" style="text-align: center;">Person-to-Camera Distance Variation</h2> -->
            <div class="columns is-centered is-multiline">
              <div class="column">
                <video width="960" height="720" controls="">
                  <source src="generate_images/videos/1.mp4" type="video/mp4" width="960" height="360">
                  <source src="movie.ogg" type="video/ogg">
                  <source src="movie.webm" type="video/webm">
                  <object data="./AnyScene_files/motion_type1.mp4" width="960" height="720">
                    <embed src="movie.swf" width="960" height="720">
                  </object>
                </video>
              </div>
            </div>
            <!-- <h2 class="title is-4" style="text-align: center;">Parallel Motion Relative to the Camera</h2> -->
              <div class="columns is-centered is-multiline">
              <div class="column">
              <video width="960" height="720" controls="">
                <source src="generate_images/videos/2.mp4" type="video/mp4" width="960" height="720">
                <source src="movie.ogg" type="video/ogg">
                <source src="movie.webm" type="video/webm">
                <object data="./AnyScene_files/motion_type2.mp4" width="960" height="720">
                  <embed src="movie.swf" width="960" height="720">
                </object>
              </video>
              </div>
            </div>
            <!-- <h2 class="title is-4" style="text-align: center;">Camera Angle Dynamics</h2> -->
            <div class="columns is-centered is-multiline">
              <div class="column">
              <video width="960" height="720" controls="">
                <source src="generate_images/videos/3.mp4" type="video/mp4" width="960" height="720">
                <source src="movie.ogg" type="video/ogg">
                <source src="movie.webm" type="video/webm">
                <object data="./AnyScene_files/motion_type3.mp4" width="960" height="720">
                  <embed src="movie.swf" width="960" height="720">
                </object>
              </video>
              </div>
            </div>


        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <h2 class="title is-3" style="text-align: center;">Adapting to Different Clothing Styles</h2> -->
              <!-- <div class="content has-text-justified">
                <p>
                  Unlike previous video try-on methods limited to fitting tight-fitting tops, Our Tunnel Try-on can perform try-on tasks for different types of tops and bottoms.
                </p>
              </div> -->
            <div class="columns is-centered has-text-centered">
              <div class="column">
                <video width="960" height="720" controls="">
                  <source src="generate_images/videos/4.mp4" type="video/mp4" width="960" height="720">
                  <source src="movie.ogg" type="video/ogg">
                  <source src="movie.webm" type="video/webm">
                  <object data="./AnyScene_files/appendix_figure8_a_tops2tops.mp4" width="960" height="720">
                    <embed src="movie.swf" width="960" height="720">
                  </object>
                </video>
              </div>
            </div>




</div></div></div></div></div></div></div></section>


<!-- <section class="section">
  <div class="container is-max-desktop"> -->
    <!-- Abstract. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video try-on is a challenging task and has not been well tackled in previous works. The main obstacle lies in preserving the details of the clothing and modeling the coherent motions simultaneously.

            Faced with those difficulties, we address video try-on by proposing a diffusion-based framework named "<b>Tunnel Try-on</b>".  

            The core idea is excavating a ``focus tunnel'' in the input video that gives close-up shots around the clothing regions. We zoom in on the region in the tunnel to better preserve the fine details of the clothing.

            To generate coherent motions, we first leverage the Kalman filter to construct smooth crops in the focus tunnel and inject the position embedding of the tunnel into attention layers to improve the continuity of the generated videos.

            In addition, we develop an environment encoder to extract the context information outside the tunnels as supplementary cues.

            Equipped with these techniques, Tunnel Try-on keeps the fine details of the clothing and synthesizes stable and smooth videos.

            Demonstrating significant advancements, Tunnel Try-on could be regarded as the first attempt toward the commercial-level application of virtual try-on in videos.
          </p>
          </p>
        </div>
      </div>
    </div>

  </div>
</section> -->





<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xu2024tunnel,
      title={Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos},
      author={Xu, Zhengze and Chen, Mengting and Wang, Zhao and Xing, Linyu and Zhai, Zhonghua and Sang, Nong and Lan, Jinsong and Xiao, Shuai and Gao, Changxin},
      journal={arXiv preprint},
      year={2024}
    }</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="file:///Users/yms/Documents/AnyScene-v1/index.html">
        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
      </a>
      
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </p>
      </div>
    </div>
  </div>
</footer>



</body></html>